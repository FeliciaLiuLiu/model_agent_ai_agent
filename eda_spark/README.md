# EDA Spark (CML)

Spark-first EDA for large datasets and CML environments. This version reads the mixed AML dataset generated by `scripts/07_generate_synthetic_aml_mixed_bank_fintech.py` and uses PySpark for all computations where possible, only converting small aggregates to pandas for plotting.

## Minimal Files to Copy into a Fresh CML Project

You only need:
- `eda_spark/` (this folder)
- `data/` (your datasets)
- `eda_spark/requirements.txt` (dependencies)

Important: keep the folder name **lowercase** `eda_spark` (Linux is case-sensitive).

## Quick Start (One-Shot Report)

1) Generate the latest mixed AML dataset:
```bash
python scripts/07_generate_synthetic_aml_mixed_bank_fintech.py
```

2) Run EDA Spark and export the PDF + JSON:
```bash
export MPLCONFIGDIR=./.mpl_cache
export MPLBACKEND=Agg

python -m eda_spark.cli \
  --output ./output_eda_spark
```

Outputs:
- `./output_eda_spark/EDA_Report.pdf`
- `./output_eda_spark/eda_results.json`

By default, the CLI auto-detects the latest `synthetic_aml_mixed_50k_*.csv` under `./data`. You can override the dataset:
```bash
python -m eda_spark.cli --data ./data/synthetic_aml_mixed_50k_YYYYMMDD_HHMMSS.csv
```
If you want parquet, pass it explicitly via `--data`.
Local paths are forced to `file://` URIs internally to avoid HDFS defaults in CML.

## Interactive Mode (Choose Sections + Columns)

```bash
python -m eda_spark.cli --interactive
```

How interactive mode works:
- You will be prompted to select sections by number (e.g., `1,3,5`) or `all`.
- For each selected section, you can choose columns by number or name.
- Press Enter to accept the defaults.

You can list all sections and descriptions:
```bash
python -m eda_spark.cli --list-functions
```

## Notebook Usage (Non-Interactive + Interactive)

Non-interactive (Notebook):
```python
import sys
sys.path.append(".")

from eda_spark.runner import EDASpark

eda = EDASpark(output_dir="./output_eda_spark", spark_master="local[*]")
eda.run()  # auto-detects latest synthetic_aml_mixed_50k_*.csv
```

Interactive (Notebook):
```python
import sys
sys.path.append(".")

from eda_spark.runner import EDASpark

eda = EDASpark(output_dir="./output_eda_spark", spark_master="local[*]")
eda.run_interactive()
```

## CLI Usage (Module or Script)

Both styles work:
```bash
python -m eda_spark.cli --output ./output_eda_spark
```
```bash
python eda_spark/cli.py --output ./output_eda_spark
```

## Section and Column Overrides (Non-Interactive)

Run only specific sections:
```bash
python -m eda_spark.cli --sections data_quality,target,univariate
```

Provide columns globally:
```bash
python -m eda_spark.cli --columns txn_amount,velocity_score,origin_country
```

Or per section:
```bash
python -m eda_spark.cli \
  --columns-univariate txn_amount,velocity_score \
  --columns-target sar_actual
```

## CML Notes

Typical CML Spark session environment configuration:
```bash
export JAVA_TOOL_OPTIONS="-Djava.io.tmpdir=./spark_tmp"
export SPARK_LOCAL_DIRS="./spark_tmp"
export SPARK_DRIVER_OPTS="-Djava.io.tmpdir=./spark_tmp"
export SPARK_EXECUTOR_OPTS="-Djava.io.tmpdir=./spark_tmp"
export MPLCONFIGDIR=./.mpl_cache
export MPLBACKEND=Agg

python -m eda_spark.cli --output ./output_eda_spark --spark-master "local[*]"
```

You can also pass extra Spark configs:
```bash
python -m eda_spark.cli \
  --spark-conf spark.sql.shuffle.partitions=200 \
  --spark-conf spark.driver.memory=4g
```

## Environment Overrides

- `EDA_SPARK_DATA_PATH`: force a specific dataset path.
- `MPLCONFIGDIR`: Matplotlib cache path (recommended for CML).
- `MPLBACKEND=Agg`: headless PDF rendering.
