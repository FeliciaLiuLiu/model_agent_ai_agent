# EDA Spark (CML)

Spark-first EDA for large datasets and CML environments. This version reads the mixed AML dataset generated by `scripts/07_generate_synthetic_aml_mixed_bank_fintech.py` and uses PySpark for all computations where possible, only converting small aggregates to pandas for plotting.

## Quick Start (One-Shot Report)

1) Generate the latest mixed AML dataset:
```bash
python scripts/07_generate_synthetic_aml_mixed_bank_fintech.py
```

2) Run EDA Spark and export the PDF + JSON:
```bash
export MPLCONFIGDIR=./.mpl_cache
export MPLBACKEND=Agg

python -m eda_spark.cli \
  --output ./output_eda_spark
```

Outputs:
- `./output_eda_spark/EDA_Report.pdf`
- `./output_eda_spark/eda_results.json`

By default, the CLI auto-detects the latest `synthetic_aml_mixed_50k_*.csv|parquet` under `./data`. You can override the dataset:
```bash
python -m eda_spark.cli --data ./data/synthetic_aml_mixed_50k_YYYYMMDD_HHMMSS.csv
```

## Interactive Mode (Choose Sections + Columns)

```bash
python -m eda_spark.cli --interactive
```

How interactive mode works:
- You will be prompted to select sections by number (e.g., `1,3,5`) or `all`.
- For each selected section, you can choose columns by number or name.
- Press Enter to accept the defaults.

You can list all sections and descriptions:
```bash
python -m eda_spark.cli --list-functions
```

## Section and Column Overrides (Non-Interactive)

Run only specific sections:
```bash
python -m eda_spark.cli --sections data_quality,target,univariate
```

Provide columns globally:
```bash
python -m eda_spark.cli --columns txn_amount,velocity_score,origin_country
```

Or per section:
```bash
python -m eda_spark.cli \
  --columns-univariate txn_amount,velocity_score \
  --columns-target sar_actual
```

## CML Notes

Typical CML Spark session environment configuration:
```bash
export JAVA_TOOL_OPTIONS="-Djava.io.tmpdir=./spark_tmp"
export SPARK_LOCAL_DIRS="./spark_tmp"
export SPARK_DRIVER_OPTS="-Djava.io.tmpdir=./spark_tmp"
export SPARK_EXECUTOR_OPTS="-Djava.io.tmpdir=./spark_tmp"
export MPLCONFIGDIR=./.mpl_cache
export MPLBACKEND=Agg

python -m eda_spark.cli --output ./output_eda_spark --spark-master "local[*]"
```

You can also pass extra Spark configs:
```bash
python -m eda_spark.cli \
  --spark-conf spark.sql.shuffle.partitions=200 \
  --spark-conf spark.driver.memory=4g
```

## Environment Overrides

- `EDA_SPARK_DATA_PATH`: force a specific dataset path.
- `MPLCONFIGDIR`: Matplotlib cache path (recommended for CML).
- `MPLBACKEND=Agg`: headless PDF rendering.
